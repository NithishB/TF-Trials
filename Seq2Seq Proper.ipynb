{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq():\n",
    "    def __init__(self,params,vocab_to_int,embeddings):\n",
    "        self.params = params\n",
    "        self.vocab_to_int = vocab_to_int\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def _init_placeholders(self):\n",
    "        with tf.name_scope(\"Input_Placeholder\"):\n",
    "            self.encoder_inputs = tf.placeholder(dtype=tf.int32, shape=(None,None), name='encoder_inputs')\n",
    "            self.decoder_targets = tf.placeholder(dtype=tf.int32, shape=(None,None), name='decoder_targets')\n",
    "            self.encoder_lengths = tf.placeholder(dtype=tf.int32, shape=(None,), name='encoder_lengths')\n",
    "            self.decoder_lengths = tf.placeholder(dtype=tf.int32, shape=(None,), name='decoder_lengths')\n",
    "            self.max_dec_length = tf.reduce_max(self.decoder_lengths, name='max_dec_len')\n",
    "            self.learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "            \n",
    "    def _single_rnn_(self, cell_type, num_units):\n",
    "        cell_type = cell_type.upper()\n",
    "        if cell_type == 'LSTM':\n",
    "            return tf.contrib.rnn.LSTMCell(num_units,initializer=tf.truncated_normal_initializer(-0.1, 0.1, seed=2))\n",
    "        elif cell_type == 'GRU':\n",
    "            return tf.contrib.rnn.GRUCell(num_units,initializer=tf.truncated_normal_initializer(-0.1, 0.1, seed=2))\n",
    "        elif cell_type == 'RNN':\n",
    "            return tf.contrib.rnn.RNNCell(num_units,initializer=tf.truncated_normal_initializer(-0.1, 0.1, seed=2))\n",
    "        elif cell_type == 'GLSTM':\n",
    "            return tf.contrib.rnn.GLSTMCell(num_units,initializer=tf.truncated_normal_initializer(-0.1, 0.1, seed=2))\n",
    "    \n",
    "    def _multi_rnn_(self, cell_type, num_units, num_layers):\n",
    "        return [self._single_rnn_(cell_type, num_units) for _ in range(num_layers)]\n",
    "    \n",
    "    def _encoder_(self):\n",
    "        with tf.variable_scope(\"Simple_Encoder\",reuse=tf.AUTO_REUSE):\n",
    "            enc_embed_input = tf.nn.embedding_lookup(self.embeddings, self.encoder_inputs)\n",
    "            if self.params['num_layers'] == 1:\n",
    "                cells = self._single_rnn_(self.params['cell_type'],self.params['num_units'])\n",
    "                self.encoder_outputs, self.encoder_final_state = tf.nn.dynamic_rnn(cell=cells,inputs=enc_embed_input,sequence_length=self.encoder_lengths,dtype=tf.float32) \n",
    "            else:\n",
    "                cells = tf.contrib.rnn.MultiRNNCell(self._multi_rnn_(self.params['cell_type'],self.params['num_units'],self.params['num_layers']))\n",
    "                self.encoder_outputs, self.encoder_states = tf.nn.dynamic_rnn(cell=cells,inputs=enc_embed_input,sequence_length=self.encoder_lengths,dtype=tf.float32)\n",
    "                self.encoder_final_state = states[-1]\n",
    "            \n",
    "    def _bidirectional_encoder_(self):\n",
    "        with tf.variable_scope(\"Bidirectional_Encoder\",reuse=tf.AUTO_REUSE):\n",
    "            enc_embed_input = tf.nn.embedding_lookup(self.embeddings, self.encoder_inputs)\n",
    "            if self.params['num_layers'] == 1:\n",
    "                cells_fw = self._single_rnn_(self.params['cell_type'],self.params['num_units'])\n",
    "                cells_bw = self._single_rnn_(self.params['cell_type'],self.params['num_units'])\n",
    "            else:\n",
    "                cells_fw = self._multi_rnn_(self.params['cell_type'],self.params['num_units'],self.params['num_layers'])\n",
    "                cells_bw = self._multi_rnn_(self.params['cell_type'],self.params['num_units'],self.params['num_layers'])\n",
    "            outputs, encoder_final_fw_states, encoder_final_bw_states = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw=cells_fw,cells_bw=cells_bw,inputs=enc_embed_input,sequence_length=self.encoder_lengths,dtype=tf.float32) \n",
    "            self.encoder_outputs = tf.concat(outputs,axis=2,name='Encoder_BiDirectional_Output_Concat')\n",
    "            c = tf.reduce_mean([tf.concat((encoder_final_fw_states[i].c, encoder_final_bw_states[i].c),1) for i in range(self.params['num_layers'])],axis=0,name='Encoder_Bidirectional_State_Concat_c')\n",
    "            h = tf.reduce_mean([tf.concat((encoder_final_fw_states[i].h, encoder_final_bw_states[i].h),1) for i in range(self.params['num_layers'])],axis=0,name='Encoder_Bidirectional_State_Concat_h')\n",
    "            self.encoder_final_state = tf.contrib.rnn.LSTMStateTuple(c,h)\n",
    "    \n",
    "    def _decoder_with_attention_(self,infer=False):\n",
    "        if self.params['bidirectional']:\n",
    "            if self.params['num_layers'] == 1:\n",
    "                cells = self._single_rnn_(self.params['cell_type'],self.params['num_units']*2)\n",
    "            else:\n",
    "                cells = self._multi_rnn_(self.params['cell_type'],self.params['num_units']*2,self.params['num_layers'])\n",
    "        else:\n",
    "            if self.params['num_layers'] == 1:\n",
    "                cells = self._single_rnn_(self.params['cell_type'],self.params['num_units'])\n",
    "            else:\n",
    "                cells = self._multi_rnn_(self.params['cell_type'],self.params['num_units'],self.params['num_layers'])\n",
    "        if infer:\n",
    "            memory = tf.contrib.seq2seq.tile_batch(self.encoder_outputs, self.params['beam_size'])\n",
    "            memory_sequence_length = tf.contrib.seq2seq.tile_batch(self.encoder_lengths, self.params['beam_size'])\n",
    "        else:\n",
    "            memory = self.encoder_outputs\n",
    "            memory_sequence_length = self.encoder_lengths\n",
    "        \n",
    "        if self.params['attention'] == 'bahdanau':\n",
    "            attn_mech = tf.contrib.seq2seq.BahdanauAttention(num_units=self.params['num_units'],memory=memory,memory_sequence_length=memory_sequence_length)\n",
    "        elif self.params['attention'] == 'bahdanau_norm':\n",
    "            attn_mech = tf.contrib.seq2seq.BahdanauAttention(num_units=self.params['num_units'],memory=memory,memory_sequence_length=memory_sequence_length,normalize=True)\n",
    "        elif self.params['attention'] == 'luong':\n",
    "            attn_mech = tf.contrib.seq2seq.LuongAttention(num_units=self.params['num_units'],memory=memory,memory_sequence_length=memory_sequence_length)\n",
    "        elif self.params['attention'] == 'luong_scaled':\n",
    "            attn_mech = tf.contrib.seq2seq.LuongAttention(num_units=self.params['num_units'],memory=memory,memory_sequence_length=memory_sequence_length,scale=True)\n",
    "        \n",
    "        cells[0] = tf.contrib.seq2seq.AttentionWrapper(cell=cells[0],attention_mechanism=attn_mech,attention_layer_size=self.params['num_units'])\n",
    "        \n",
    "        if infer:\n",
    "            batch_size = self.params['batch_size']*self.params['beam_size']\n",
    "            self.encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_final_state,self.params['beam_size'])\n",
    "        else:\n",
    "            batch_size = self.params['batch_size']\n",
    "        initial_state = [self.encoder_final_state for i in range(self.params['num_layers'])]\n",
    "        attention_cell_state = cells[0].zero_state(dtype=tf.float32,batch_size=batch_size)\n",
    "        initial_state[0] = attention_cell_state.clone(cell_state=initial_state[0])\n",
    "        self.decoder_initial_state = tuple(initial_state)\n",
    "        self.decoder_cells = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        self.output_layer = Dense(self.params['vocab_size'],kernel_initializer=tf.truncated_normal_initializer(mean = 0.0, stddev=0.1),name='Output')\n",
    "    \n",
    "    def _train_decoder_(self):\n",
    "        with tf.variable_scope(\"Decode\",reuse=tf.AUTO_REUSE):\n",
    "            self._decoder_with_attention_()\n",
    "            ending = tf.strided_slice(self.decoder_targets,begin=[0,0],end=[self.params['batch_size'],-1],strides=[1,1])\n",
    "            self.decoder_inputs = tf.concat([tf.fill([self.params['batch_size'], 1],self.vocab_to_int['<GO>']),ending],1)\n",
    "            dec_embed_input = tf.nn.embedding_lookup(self.embeddings, self.decoder_inputs)\n",
    "\n",
    "            train_helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input,sequence_length=self.decoder_lengths,name='Train_Helper')\n",
    "            self.train_decoder = tf.contrib.seq2seq.BasicDecoder(cell=self.decoder_cells,helper=train_helper,initial_state=self.decoder_initial_state,output_layer=self.output_layer)\n",
    "            self.training_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(self.train_decoder,impute_finished=True,maximum_iterations=self.max_dec_length,swap_memory=True)\n",
    "\n",
    "    def _inference_decoder_(self):\n",
    "        with tf.variable_scope(\"Decode\",reuse=tf.AUTO_REUSE):\n",
    "            self._decoder_with_attention_(infer=True)\n",
    "            start_tokens = tf.fill([self.params['batch_size']], self.vocab_to_int[self.params['start_token']])\n",
    "            beam_search_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=self.decoder_cells,embedding=self.embeddings,start_tokens=start_tokens,end_token=self.vocab_to_int[self.params['end_token']],initial_state=self.decoder_initial_state,beam_width=self.params['beam_size'],output_layer=self.output_layer)\n",
    "            final_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(beam_search_decoder,maximum_iterations=self.max_dec_length)\n",
    "            beam_predictions = final_outputs.predicted_ids\n",
    "            self.inference_logits = tf.transpose(beam_predictions, perm=[0, 2, 1])\n",
    "        \n",
    "    def graph_construct(self):\n",
    "        train_graph = tf.Graph()\n",
    "        with train_graph.as_default():\n",
    "            self._init_placeholders()\n",
    "            print(\"Initialized Placeholders\")\n",
    "            if self.params['bidirectional']:\n",
    "                self._bidirectional_encoder_()\n",
    "            else:\n",
    "                self._encoder_()\n",
    "            print(\"Encoder Netwrok Constructed\")\n",
    "            self._train_decoder_()\n",
    "            print(\"Train Decoder Netwrok Constructed\")\n",
    "            self._inference_decoder_()\n",
    "            print(\"Infer Decoder Netwrok Constructed\")\n",
    "            training_logits = tf.identity(self.training_logits.rnn_output, 'logits')\n",
    "            inference_logits = tf.identity(self.inference_logits, name='predictions')\n",
    "            masks = tf.sequence_mask(self.decoder_lengths,self.max_dec_length,dtype=tf.float32,name='masks')\n",
    "            with tf.name_scope(\"optimization\"):\n",
    "                cost = tf.contrib.seq2seq.sequence_loss(training_logits,self.decoder_targets,masks)\n",
    "                optimizer = tf.train.AdamOptimizer(self.params['learning_rate'])\n",
    "                # Gradient Clipping\n",
    "                gradients = optimizer.compute_gradients(cost)\n",
    "                capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "                train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        print(\"Graph is built.\")\n",
    "        return train_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_units':128,\n",
    "          'num_layers':3,\n",
    "          'vocab_size':1000,\n",
    "          'embed_dim':300,\n",
    "          'cell_type':'LSTM',\n",
    "          'attention':'bahdanau',\n",
    "          'batch_size':32,\n",
    "          'bidirectional':True,\n",
    "          'beam_size':10,\n",
    "          'end_token':'<EOS>',\n",
    "          'start_token':'<GO>',\n",
    "          'learning_rate':0.001}\n",
    "word2int = {'<GO>':1,'<EOS>':2,'<PAD>':3}\n",
    "embedding_matrix = np.ones(shape=(params['vocab_size'],params['embed_dim']),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqob = Seq2seq(params,word2int,embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Placeholders\n",
      "Encoder Netwrok Constructed\n",
      "Train Decoder Netwrok Constructed\n",
      "Infer Decoder Netwrok Constructed\n",
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "graph = seqob.graph_construct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(\"./logs/seq2seq_2/run1\", sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
