{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq():\n",
    "\n",
    "    def __init__(self,enc_units,num_enc_layer,num_dec_layer,vocab_size,embed_dim,batch_size,word2int,beam_width,bidirectional=True,cell_type='LSTM',attention='bahdanau',decode_mech='Greedy',coverage=False):\n",
    "        self.encoder_units = enc_units\n",
    "        self.encoder_layers = num_enc_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.cell_type = cell_type\n",
    "        self.decoder_units = self.encoder_units*2 if bidirectional else self.encoder_units\n",
    "        self.decoder_layers = num_dec_layer\n",
    "        self.attention = attention\n",
    "        self.decode_mechanism = decode_mech\n",
    "        self.beam_width = beam_width\n",
    "        self.use_coverage = coverage\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_to_int = word2int\n",
    "\n",
    "    def _init_placeholders(self):\n",
    "        # Creates placeholders for encoder and decoder inputs and lengths\n",
    "        with tf.variable_scope(\"Placeholders\", reuse = tf.AUTO_REUSE):    \n",
    "            self.encoder_inputs = tf.placeholder(dtype=tf.int32, shape=(None,None), name='encoder_inputs')\n",
    "            self.decoder_targets = tf.placeholder(dtype=tf.int32, shape=(None,None), name='decoder_targets')\n",
    "            self.encoder_lengths = tf.placeholder(dtype=tf.int32, shape=(None,), name='encoder_lengths')\n",
    "            self.decoder_lengths = tf.placeholder(dtype=tf.int32, shape=(None,), name='decoder_lengths')\n",
    "            self.max_dec_length = tf.reduce_max(self.decoder_lengths, name='max_dec_len')\n",
    "            self.lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "            self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    def _decoder_input_processing(self):\n",
    "        with tf.variable_scope(\"Processing\", reuse=tf.AUTO_REUSE):\n",
    "            ending = tf.strided_slice(self.decoder_targets,begin=[0,0],end=[self.batch_size,-1],strides=[1,1])\n",
    "            self.decoder_inputs = tf.concat([tf.fill([self.batch_size, 1],self.vocab_to_int['<GO>']),ending],1)\n",
    "            \n",
    "    def _create_encoder(self):\n",
    "        with tf.variable_scope(\"Encoder_Layer\", reuse=tf.AUTO_REUSE):\n",
    "            if self.cell_type == 'LSTM':\n",
    "                self.cells_fw = [tf.contrib.rnn.LSTMCell(self.encoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.encoder_layers)]\n",
    "                if self.bidirectional:\n",
    "                    self.cells_bw = [tf.contrib.rnn.LSTMCell(self.encoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.encoder_layers)]\n",
    "            elif self.cell_type == 'GRU':\n",
    "                self.cells_fw = [tf.contrib.rnn.GRUCell(self.encoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.encoder_layers)]\n",
    "                if self.bidirectional:\n",
    "                    self.cells_bw = [tf.contrib.rnn.GRUCell(self.encoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.encoder_layers)]\n",
    "            elif self.cell_type == 'GLSTM':\n",
    "                self.cells_fw = [tf.contrib.rnn.GLSTMCell(self.encoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.encoder_layers)]\n",
    "                if self.bidirectional:\n",
    "                    self.cells_bw = [tf.contrib.rnn.GLSTMCell(self.encoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.encoder_layers)]\n",
    "            elif self.cell_type == 'RNN':\n",
    "                self.cells_fw = [tf.contrib.rnn.RNNCell(self.encoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.encoder_layers)]\n",
    "                if self.bidirectional:\n",
    "                    self.cells_bw = [tf.contrib.rnn.RNNCell(self.encoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.encoder_layers)]\n",
    "    \n",
    "    def _create_decoder(self):\n",
    "        with tf.variable_scope(\"Decoder_Layer\",reuse=tf.AUTO_REUSE):\n",
    "            if self.cell_type == 'LSTM':\n",
    "                dec_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(self.decoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.decoder_layers)])\n",
    "            elif self.cell_type == 'GRU':\n",
    "                dec_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(self.decoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.decoder_layers)])\n",
    "            elif self.cell_type == 'GLSTM':\n",
    "                dec_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GLSTMCell(self.decoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.decoder_layers)])\n",
    "            elif self.cell_type == 'RNN':\n",
    "                dec_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.RNNCell(self.decoder_units,initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2)) for _ in range(self.decoder_layers)])\n",
    "            \n",
    "            dec_cells = tf.contrib.rnn.DropoutWrapper(dec_cells,input_keep_prob=0.8)\n",
    "            self.output_layer = Dense(self.vocab_size,kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1),name='Output')\n",
    "            \n",
    "            if self.attention != None:\n",
    "                if self.attention == 'bahdanau':\n",
    "                    attn_mech = tf.contrib.seq2seq.BahdanauAttention(self.decoder_units,\n",
    "                                                                     self.enc_output,\n",
    "                                                                     self.encoder_lengths,\n",
    "                                                                     normalize=False,\n",
    "                                                                     name='BahdanauAttention')\n",
    "                elif self.attention =='luong':\n",
    "                    attn_mech = tf.contrib.seq2seq.LuongAttention(self.decoder_units,\n",
    "                                                                  self.enc_output,\n",
    "                                                                  self.encoder_lengths,\n",
    "                                                                  name='LuongAttention')\n",
    "                self.dec_cells = tf.contrib.seq2seq.AttentionWrapper(dec_cells,attn_mech,self.decoder_units)\n",
    "                init_state = self.dec_cells.zero_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "                self.initial_state = init_state.clone(cell_state=self.enc_state)\n",
    "            else:\n",
    "                self.dec_cells = dec_cells\n",
    "                self.initial_state = self.enc_state\n",
    "            \n",
    "    \n",
    "    def encoding(self,embeddings):\n",
    "        with tf.variable_scope(\"Encoding\",reuse=tf.AUTO_REUSE):\n",
    "            self._create_encoder()\n",
    "            enc_embed_input = tf.nn.embedding_lookup(embeddings, self.encoder_inputs)\n",
    "            if self.bidirectional:\n",
    "                enc_output,enc_fw_state,enc_bw_state = \\\n",
    "                tf.contrib.rnn.stack_bidirectional_dynamic_rnn(self.cells_fw,\n",
    "                                                               self.cells_bw,\n",
    "                                                               enc_embed_input,\n",
    "                                                               sequence_length=self.encoder_lengths,\n",
    "                                                               dtype=tf.float32)\n",
    "\n",
    "                self.enc_output = tf.concat(enc_output,2)\n",
    "                if(self.encoder_layers > 1):\n",
    "                    if isinstance(enc_fw_state[0], tf.contrib.rnn.LSTMStateTuple):\n",
    "                        enc_state_c = tf.reduce_mean([tf.concat((enc_fw_state[i].c, enc_bw_state[i].c) , 1) for i in range(self.encoder_layers)], axis=0, name='bidirectional_concat_c')\n",
    "                        enc_state_h = tf.reduce_mean([tf.concat((enc_fw_state[i].h, enc_bw_state[i].h), 1) for i in range(self.encoder_layers)], axis=0, name='bidirectional_concat_c')\n",
    "                        self.enc_state = (tf.contrib.rnn.LSTMStateTuple(c=enc_state_c, h=enc_state_h),)\n",
    "\n",
    "                    elif isinstance(enc_fw_state[0], tf.Tensor):\n",
    "                        self.enc_state = (tf.concat((enc_fw_state, enc_bw_state),1, name='bidirectional_concat'),)\n",
    "                else:\n",
    "                    if isinstance(enc_fw_state, tf.contrib.rnn.LSTMStateTuple):\n",
    "                        enc_state_c = tf.concat((enc_fw_state.c, enc_bw_state.c), 1, name='bidirectional_concat_c')\n",
    "                        enc_state_h = tf.concat((enc_fw_state.h, enc_bw_state.h), 1, name='bidirectional_concat_h')\n",
    "                        self.enc_state = (tf.contrib.rnn.LSTMStateTuple(c=enc_state_c, h=enc_state_h),)\n",
    "\n",
    "                    elif isinstance(enc_fw_state, tf.Tensor):\n",
    "                        self.enc_state = (tf.concat((enc_fw_state, enc_bw_state), 1, name='bidirectional_concat'),)\n",
    "\n",
    "            else:\n",
    "                self.enc_output, self.enc_state = tf.nn.dynamic_rnn(self.cells_fw,\n",
    "                                                                    enc_embed_input,\n",
    "                                                                    self.encoder_lengths,\n",
    "                                                                    dtype=tf.float32)\n",
    "    \n",
    "    def decoding(self,embeddings):\n",
    "        with tf.variable_scope(\"Decoding\", reuse=tf.AUTO_REUSE):\n",
    "            self._create_decoder()\n",
    "            self._decoder_input_processing()\n",
    "            dec_embed_input = tf.nn.embedding_lookup(embeddings, self.decoder_inputs)\n",
    "            train_helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input,\n",
    "                                                             sequence_length=self.decoder_lengths,\n",
    "                                                             name='train_helper')\n",
    "            self.train_decoder = tf.contrib.seq2seq.BasicDecoder(cell=self.dec_cells,\n",
    "                                                                 helper=train_helper,\n",
    "                                                                 initial_state=self.initial_state,\n",
    "                                                                 output_layer=self.output_layer)\n",
    "\n",
    "            start_tokens = tf.tile(tf.constant([self.vocab_to_int['<GO>']],dtype=tf.int32),[self.batch_size],name='start_tokens')\n",
    "\n",
    "            if self.decode_mechanism == 'Greedy':\n",
    "                inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                            start_tokens,\n",
    "                                                                            self.vocab_to_int['<EOS>'])\n",
    "\n",
    "            self.inference_decoder = tf.contrib.seq2seq.BasicDecoder(self.dec_cells,\n",
    "                                                                     inference_helper,\n",
    "                                                                     self.initial_state,\n",
    "                                                                     self.output_layer)\n",
    "\n",
    "            self.training_logits, training_state, training_lengths = \\\n",
    "            tf.contrib.seq2seq.dynamic_decode(self.train_decoder,\n",
    "                                              impute_finished=True,\n",
    "                                              maximum_iterations=self.max_dec_length)\n",
    "\n",
    "            self.inference_logits, inference_state, inference_lengths = \\\n",
    "            tf.contrib.seq2seq.dynamic_decode(self.inference_decoder,\n",
    "                                              impute_finished=True,\n",
    "                                              maximum_iterations=self.max_dec_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_gen():\n",
    "    enc_units = 128\n",
    "    num_enc_layer = 3\n",
    "    num_dec_layer = 1\n",
    "    vocab_size = 1000\n",
    "    embed_dim = 300\n",
    "    batch_size = 32\n",
    "    word2int = {'<GO>':1,'<EOS>':2,'<PAD>':3}\n",
    "    bidirectional = True\n",
    "    cell_type = 'LSTM'\n",
    "    attention = 'bahdanau'\n",
    "    beam_width = 10\n",
    "    decode_mech = 'Greedy'\n",
    "    coverage = False\n",
    "    embedding_matrix = np.ones(shape=(vocab_size,embed_dim),dtype=np.float32)\n",
    "    \n",
    "    seq2seq_ob = Seq2Seq(enc_units=enc_units,num_enc_layer=num_enc_layer,num_dec_layer=num_dec_layer,beam_width=beam_width,vocab_size=vocab_size,embed_dim=embed_dim,word2int=word2int,batch_size=batch_size,bidirectional=bidirectional,cell_type=cell_type,attention=attention,decode_mech=decode_mech,coverage=coverage)\n",
    "    seq2seq_ob._init_placeholders()\n",
    "    seq2seq_ob._create_encoder()\n",
    "    seq2seq_ob.encoding(embedding_matrix)\n",
    "    seq2seq_ob._create_decoder()\n",
    "    seq2seq_ob.decoding(embedding_matrix)\n",
    "    \n",
    "    return seq2seq_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.005\n",
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    model_ob = model_gen()\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(model_ob.training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(model_ob.inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(model_ob.decoder_lengths,model_ob.max_dec_length,dtype=tf.float32,name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits,model_ob.decoder_targets,masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 10 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(train_summaries, train_texts, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!')\n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break\n",
    "    \n",
    "    logits = []\n",
    "    for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                  get_batches(test_summaries, test_summaries, batch_size)):\n",
    "        inf_logits = sess.run(inference_logits,\n",
    "                              {input_data: texts_batch,\n",
    "                               summary_length: summaries_lengths,\n",
    "                               text_length: texts_lengths,\n",
    "                               keep_prob: keep_probability})\n",
    "        logits.extend(inf_logits.tolist())\n",
    "      \n",
    "    logits_tr = []\n",
    "    for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "        get_batches(train_summaries, train_texts, batch_size)):\n",
    "        start_time = time.time()\n",
    "        inf_logits = sess.run(inference_logits,\n",
    "                              {input_data: texts_batch,\n",
    "                               targets: summaries_batch,\n",
    "                               lr: learning_rate,\n",
    "                               summary_length: summaries_lengths,\n",
    "                               text_length: texts_lengths,\n",
    "                               keep_prob: keep_probability})\n",
    "        logits_tr.extend(inf_logits.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
